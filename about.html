<!doctype html>
<html lang="en">
  <head>
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta charset="utf-8">
    <meta content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" name="viewport">

    <!-- Use title if it's in the page YAML frontmatter -->
    <title>About VC3 | VC3</title>

    <link href='https://fonts.googleapis.com/css?family=Lato:400,100,100italic,300italic,300,400italic,700italic,700,900,900italic' rel='stylesheet' type='text/css'>

    <link href="/stylesheets/vcc.css" rel="stylesheet" />
    <script src="/javascripts/jquery-3.1.0.min.js"></script>
    <script src="/javascripts/bootstrap.min.js"></script>
    <script src="//stephband.info/jquery.event.move/js/jquery.event.move.js"></script>
    <script src="//stephband.info/jquery.event.swipe/js/jquery.event.swipe.js"></script>
    <script src="/assets/unslider/js/unslider-min.js" type="text/javascript"></script>
    <link href="/assets/unslider/css/unslider.css" rel="stylesheet" type="text/css">
    <link href="/assets/unslider/css/unslider-dots.css" rel="stylesheet" type="text/css">
    <script src="/javascripts/vcc.js"></script>
  </head>

  <body class="about">
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <h1><a class="navbar-brand" href="/">VC3</a></h1>
        </div>
    
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav">
            <li><a href="/news.html">News</a></li>
            <li class="active"><a href="/about.html">About</a></li>
            <li><a href="/documentation/sectionOne/pageOne.html">Documentation</a></li>
            <li><a href="/team.html">Team</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right navbar-social-media-links">
            <li id="navlinks-get-project-updates">Get Project Updates:</li>
            <li><a href="http://github.com/vc3-project" id="navlinks-github">Github</a></li>
            <li><a href="http://twitter.com/virtualclusters" id="navlinks-twitter">Twitter</a></li>
          </ul>
        </div><!-- /.navbar-collapse -->

      </div><!-- /.container-fluid -->
    </nav>
      <div class="vc3-light-bg">
    <div class="container">
      <div class="row">
        <div class="col-xs-12 content">
          <h1 class="h2">About VC3</h1>
          <p>Traditional high performance and high throughput computing facilities largely assume local, centralized configuration and arbitration of resources. While this model has been adequate for serving individuals for some time, it has never been satisfactory for collaborating groups that cross organizational boundaries: a computing facility designed to meet the needs of one user community rarely meets the needs of others. The result is that users retreat to private resources, resulting in excess hardware acquisition, underutilized machines, or both. This is especially unfortunate when the distinction between facilities lies more in operating system and software configuration than fundamental hardware differences.</p>

<h3 id="the-cluster-re-imagined">The cluster, re-imagined</h3>

<p>In this project, we will aim to make research computing facilities more amenable to self-service use by broad user communities.  The organizing principle is the idea of virtual clusters for community computation (VC3) in which end users can effectively "allocate" clusters from existing facilities by requesting, for example, 200 nodes of 24 cores and 64GB RAM each, with 100TB local scratch and 10Gb connectivity to external data sources. Once allocated, the owner of the virtual cluster is to install software, load data, execute jobs, and share the allocation with collaborators.  Of course, we do not expect to unilaterally change the underlying resource management systems of existing facilities. Rather, virtual clusters will be provisioned on top of existing facilities by deploying user-level tools (e.g. HTCondor-based tools for resource management, data caching, and Parrot for virtual file system access) within the existing batch systems.  In short, a virtual cluster will appear to facility administrators as a large parallel job managed by the end user.</p>

<h3 id="smarter-workloads">Smarter workloads</h3>

<p>But virtual clusters by themselves are not enough: workloads must be self-configuring so that they can bring necessary dependencies, discover the dynamic configuration, and adapt to the (possibly) changing conditions of the virtual cluster. To accomplish this integration with reasonable effort, we will leverage a variety of existing open source tools and production quality services.  The end product will be a flexible, optimizable software ecosystem with a generic set of capabilities, presented to users as a virtual cluster service accessible via a web portal.  We presume the cluster as an optimal computing abstraction as it naturally covers a large swath of modern scientific computation and training base of expertise (e.g. campus HPC centers). We use as principal drivers computation in high-energy physics, and in particular data-intensive analysis workflows required for analysis of multi-petabyte scale datasets flowing from the ATLAS and CMS experiments at the Large Hadron Collider (LHC) at CERN in Geneva, Switzerland.</p>

<p>The deliverables for this project will include:</p>

<ul>
  <li>A reference virtual cluster platform service which describes the layered architectural components and the relationships and connections between them;</li>
  <li>A number of production service instances, providing a core computational feature set, to be used to coordinate grid, commercial cloud, HPC center, and importantly, university campus cluster access;</li>
  <li>A selection of workflow tools and services that are capable of functioning effectively in the dynamically-configured environment of a virtual cluster.</li>
</ul>

<p>The effectiveness of these deliverables will be demonstrated at scale with CMS and ATLAS user applications and the adoption of the VC3-implemented virtual clusters by new communities.</p>


        </div>
      </div>
    </div>
  </div>

    <div class="vc3-light-bg">
      <footer>
        <div class="container">
          <div class="row">
            <div class="col-xs-12">
              <a href="https://www.bnl.gov/world/"><img alt="Brookhaven National Laboratory" class="small-logo" id="brookhaven-national-laboratory-logo" src="/images/brookhaven-national-laboratory.png"/></a>
              <a href="https://www.uchicago.edu/"><img alt="The University of Chicago" class="small-logo" id="the-university-of-chicago-logo" src="/images/university-of-chicago.png"/></a>
              <a href="https://www.nd.edu/"><img alt="University of Notre Dame" class="small-logo" id="university-of-notre-dame-logo" src="/images/university-of-notre-dame.png"/></a>
              <br/>
              <a href="http://science.energy.gov/ascr/"><img alt="Department of Energy" class="doe-logo" src="/images/department-of-energy.png"/></a>
              <p class="copyright">
                &copy; 2016 Brookhaven National Laboratory, The University of Chicago, and University of Notre Dame.<br>
                Supported by the Department of Energy Office of Advanced Scientific Computing Research and Next Generation Networking Services, Solicitation  DE-­FOA­0001344 (DDRM), Proposal 0000219942.
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </body>
</html>
